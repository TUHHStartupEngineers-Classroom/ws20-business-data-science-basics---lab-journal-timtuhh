---
title: "Journal (reproducible report)"
author: "Tim Sonntag"
date: "2020-11-05"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

# Challenge 01


```{r plot, fig.width=10, fig.height=7}
# 1.0 Load libraries ----
library(tidyverse)
library(readxl)


# 2.0 Importing Files ----
bikes_tbl      <- read_excel(path = "00_data/01_bike_sales/01_raw_data/bikes.xlsx")
orderlines_tbl <- read_excel("00_data/01_bike_sales/01_raw_data/orderlines.xlsx")
bikeshops_tbl  <- read_excel("00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")

# 3.0 Examining Data ----
orderlines_tbl %>%
  head(n = 5)

glimpse(orderlines_tbl)

bikes_tbl %>%
  head(n = 5)


# 4.0 Joining Data ----

left_join(orderlines_tbl, bikes_tbl, by = c("product.id" = "bike.id"))

bike_orderlines_joined_tbl <- orderlines_tbl %>%
  left_join(bikes_tbl, by = c("product.id" = "bike.id")) %>%
  left_join(bikeshops_tbl, by = c("customer.id" = "bikeshop.id"))

glimpse(bike_orderlines_joined_tbl)


# 5.0 Wrangling Data ----

bike_orderlines_joined_tbl %>% 
  select(category) %>%
  filter(str_detect(category, "^Mountain")) %>% 
  unique()

glimpse(bike_orderlines_joined_tbl)

bike_orderlines_wrangled_tbl <- bike_orderlines_joined_tbl %>%
  #split up a column in 2 different ones
  separate(col    = location,
           into   = c("city", "state"),
           sep    = ", ") %>%
  #add ne column with calculation
  mutate(total.price = price * quantity)


glimpse(bike_orderlines_wrangled_tbl)

# prepare for plot 1 ----

sales_by_state_tbl <- bike_orderlines_wrangled_tbl %>%
  #select only the two important columns
  select(state, total.price) %>%
  group_by(state) %>%
  #aufaddieren der sales je state
  summarize(sales = sum(total.price)) %>%
  #adding a currency column
  #mutate(sales_text = scales::dollar(sales))
  mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                     decimal.mark = ",", 
                                     prefix = "", 
                                     suffix = " €"))

glimpse(sales_by_state_tbl)

# plotting plot 1 ----

sales_by_state_tbl %>%
  #creating a canvas
  ggplot(aes(x = state, y = sales)) +
  # Geometries
  geom_col(fill = "#2DC6D6") + # Use geom_col for a bar plot
  geom_label(aes(label = sales_text)) + # Adding labels to the bars
  # scale_y_continuous(labels = scales::dollar) + # Change the y-axis. 
  # Again, we have to adjust it for euro values
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title    = "Revenue by state",
    subtitle = "Upward Trend",
    x = "", # Override defaults for x and y
    y = "Revenue"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# preparing for plot 2 ----

library(lubridate)

sales_by_state_and_year_tbl <- bike_orderlines_wrangled_tbl %>%
  
  # Select columns and add a year
  select(order.date, total.price, state) %>%
  mutate(year = year(order.date)) %>%
  group_by(state, year) %>%
  summarise(sales = sum(total.price)) %>%
  ungroup() %>%
  mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                     decimal.mark = ",", 
                                     prefix = "", 
                                     suffix = " €"))

glimpse(sales_by_state_and_year_tbl)

# plotting plot 2 ----

sales_by_state_and_year_tbl %>%
  
  # Set up x, y, fill
  ggplot(aes(x = year, y = sales, fill = state)) +
  
  # Geometries
  geom_col() + # Run up to here to get a stacked bar plot
  
  # Facet
  facet_wrap(~ state) +
  
  # Formatting
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title = "Revenue by state and year",
    subtitle = "A comparison between the states",
    fill = "Main category" # Changes the legend name
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Challenge 02 
## API - Project

```{r 2}
library(httr)
library(jsonlite)  # converts JSON files to R objects
library(rstudioapi)

#API to look up which persons are currently in space

res = GET("http://api.open-notify.org/astros.json")

res

data = fromJSON(rawToChar(res$content))

data$people


#API to get time when the ISS flies by over Hamburg

res2 = GET("http://api.open-notify.org/iss-pass.json", query = list(lat = 53.6, lon = 10.0))

data2 = fromJSON(rawToChar(res2$content))

data2$response

# to view it in a normal time it hast to be converted with the anytime package from UNIX time to date time

```

## Web Scraping - Project
```{r 3}
library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing
library(magrittr)

url_home          <- "https://www.rosebikes.de/fahrräder/rennrad"
# Read in the HTML for the entire webpage
html         <- read_html(url_home)
names <- html_nodes(html, ".catalog-category-bikes__title-text") %>%
    html_text() %>%
    str_replace("\n", "") %>%
    str_remove(pattern = ".\n")
  
#length(names)
#names

prices2 <- html_nodes(html,".catalog-category-bikes__price-title") %>%
  html_text() %>%
  str_replace("\n", "") %>%
  str_remove(pattern = ".\n")

#length(prices2)
#prices2

bike_table <- tibble(names,prices2)
bike_table

```

# Challenge 03

```{r calculation, eval = F}

library(vroom)
library(data.table)
library(tidyverse)
library(magrittr)
library(readr)

# import of patent.tsv ----
col_types <- list(
  id = col_character(),
  type = col_character(),
  number = col_character(),
  country = col_character(),
  date = col_date("%Y-%m-%d"),
  abstract = col_character(),
  title = col_character(),
  kind = col_character(),
  num_claims = col_double(),
  filename = col_character(),
  withdrawn = col_double()
)

patent_tbl <- vroom(
  file       = "patent.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
patent_tbl

#oper1 <- patent_tbl[,c("number","country","date","kind")]
#oper1

#import of assignee.tsv ----
col_types <- list(
  id = col_character(),
  type = col_character(),
  name_first = col_character(),
  name_last = col_character(),
  organization = col_character()
)

assignee_tbl <- vroom(
  file       = "assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
assignee_tbl


#import of patent_assignee.tsv ----
col_types <- list(
  patent_id = col_character(),
  assignee_id = col_character(),
  location_id = col_character()
)

patent_assignee_tbl <- vroom(
  file       = "patent_assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
patent_assignee_tbl


# first task: which companies have the most patents ----

setnames(assignee_tbl, "id", "assignee_id")

combined_data <- merge(x = patent_assignee_tbl, y = assignee_tbl, 
                       by    = "assignee_id", 
                       all.x = TRUE, 
                       all.y = FALSE)
combined_data
class(combined_data)


combined_data %>% 
  filter(type == 2)%>%
  filter(!is.na(patent_id)) %>%
  count(organization, sort = T) %>%
  slice(1:10)


# second task ----

combined_data1 <- merge(x = patent_assignee_tbl, y = assignee_tbl, 
                       by    = "assignee_id", 
                       all.x = TRUE, 
                       all.y = FALSE)

setnames(patent_tbl, "id", "patent_id")

combined_data2 <- merge(x = combined_data1,y = patent_tbl,
                        by = "patent_id",
                        all.x = TRUE, 
                        all.y = FALSE)

glimpse(combined_data2) 

combined_data2 %>% 
  filter(type.x == 2)%>%
  filter(year(date)==2019)%>%
  filter(!is.na(patent_id)) %>%
  count(organization, sort = T) %>%
  slice(1:10)

# third task ----

#import of uspc.tsv
col_types <- list(
  uuid = col_character(),
  patent_id = col_character(),
  mainclass_id = col_character(),
  subclass_id = col_character(),
  sequence = col_character()
)

uspc_tbl <- vroom(
  file       = "uspc.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
uspc_tbl

combined_data3 <- merge(x = combined_data1,y = uspc_tbl,
                        by = "patent_id",
                        all.x = TRUE, 
                        all.y = FALSE)

combined_data3

#gibt top 10 firmen weltweit aus
combined_data1 %>% 
  filter(!is.na(patent_id)) %>%
  filter(!is.na(organization)) %>%
  count(organization, sort = T) %>%
  slice(1:10)

#Get the top 5 USPTO tech main classes, their patents are assigned to
  
#sorry dass ich keine idee hatte das ergebnis automatisch in einen vector zu überführen. Hätte es auch lieber anders gemacht...
filter_list <- c("International Business Machines Corporation","Samsung Electronics Co., Ltd.","Canon Kabushiki Kaisha",
                 "Sony Corporation","Kabushiki Kaisha Toshiba","General Electric Company","Hitachi, Ltd.",
                 "Intel Corporation","Fujitsu Limited","Hewlett-Packard Development Company, L.P.")

combined_data4 <- subset(combined_data3, organization %in% filter_list) 

combined_data4%>% 
  filter(!is.na(mainclass_id)) %>%
  count(mainclass_id, sort = T) %>%
  slice(1:5)

saveRDS(combined_data,"result_3_1.rds")
saveRDS(combined_data2,"result_3_2.rds")
saveRDS(combined_data4,"result_3_3.rds")


```


```{r results}
library(readr)
library(data.table)
library(vroom)
library(tidyverse)
library(magrittr)


result_3_1 <- read_rds("result_3_1.rds")
result_3_2 <- read_rds("result_3_2.rds")
result_3_3 <- read_rds("result_3_3.rds")

result_3_1
result_3_2 
result_3_3 


```


# Challenge 04
```{r plot challenge04, fig.width=11, fig.height=8}
library(tidyverse)
library(scales)
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")

glimpse(covid_data_tbl)

filter_list <- c("Germany","United_Kingdom","France","Spain","United_States_of_America")

country_data_0 <- subset(covid_data_tbl, countriesAndTerritories %in% filter_list) 

glimpse(country_data_0)

country_data_1 <- country_data_0%>%
    select(countriesAndTerritories,cases,month,year,dateRep) %>%
    mutate(date = lubridate::dmy(dateRep)) %>%
    arrange(date) %>%
    group_by(countriesAndTerritories) %>%
    mutate(case_sum = cumsum(cases)) %>%
    ungroup() 


# plotting challenge 4.1
country_data_1 %>%
  ggplot() +
  geom_line(aes(x     = date,
                y     = case_sum,
                color = countriesAndTerritories)) +
  labs(
    title = "COVID-19 confirmed cases worldwide",
    subtitle = "As of 12/02/2020",
    x = "Year 2020",
    y = "Cumulative Cases",
    color = "Continent / Country") +
  scale_color_brewer(palette = "Set3")+
  scale_y_continuous(labels = scales::comma_format(scale = 1e-6, suffix = " M",prefix=""))+
  scale_x_continuous(labels = c("January","February","March","April","May","June","July","August",
    "September","October","November","December"),breaks = seq(as.Date("2020/1/1"), by = "month", length.out = 12))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  theme(legend.position = "bottom")
  

#Challenge 4.2
country_data_3 <- covid_data_tbl %>%
  select(countriesAndTerritories,deaths,popData2019) %>%
  mutate(across(countriesAndTerritories, str_replace_all, "_", " ")) %>%
  mutate(countriesAndTerritories = case_when(
    countriesAndTerritories == "United Kingdom" ~ "UK",
    countriesAndTerritories == "United States of America" ~ "USA",
    countriesAndTerritories == "Czechia" ~ "Czech Republic",
    TRUE ~ countriesAndTerritories
  )) %>%
  group_by(countriesAndTerritories,popData2019)  %>%
  summarize(overall_deaths = sum(deaths)) %>%
  mutate(mortality = overall_deaths / popData2019) %>%
  rename(region = countriesAndTerritories)

world <- map_data("world")

combined_covid_data <- merge(x = world, y = country_data_3, 
        by    = "region", 
        all.x = TRUE, 
        all.y = FALSE)

maximum <- max(combined_covid_data$mortality, na.rm = TRUE)
combined_covid_data %>% 
  ggplot() +
  geom_map(aes(x=long,y=lat,map_id=region, fill = mortality),map = world)+
  scale_fill_gradient(low="#ffcccb", high="#940008",labels = percent,limits = c(0,maximum),breaks=c(0, 0.0005, 0.001,maximum)) +
  theme(axis.ticks = element_blank(),axis.title = element_blank(),axis.text = element_blank())+
  labs(
    title = "Confirmed COVID-19 deaths relative to the size of the population",
    subtitle = "More than 1.2 Million confirmed COVID-19 deaths worldwide",
    fill = "Mortality Rate",
    caption = str_glue("Date: 12/02/2020"))

```
